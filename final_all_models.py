# -*- coding: utf-8 -*-
"""FINAL ALL MODELS

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_9peHYnxDW3japbRq9lIiTwjYUxVYdQQ
"""

# Load the image
image = Image.open(image_path).convert("RGB")

# Define the transformation
transform = T.Compose([T.ToTensor()])

# Preprocess the image
input_image = transform(image)
input_image = input_image.unsqueeze(0)

# Add a batch dimension
# Load the Faster R-CNN model
model = fasterrcnn_resnet50_fpn
model.eval()

# Perform inference
with torch.no_grad():
predictions = model(input_image)

# Filter predictions based on confidence threshold
boxes = predictions[0]['boxes']
scores = predictions[0]['scores']
selected_boxes = boxes[scores > confidence_threshold]

# Display results with bounding boxes
fig, ax = plt.subplots(1, figsize=(10, 10))
ax.imshow(image)
for box in selected_boxes:
x, y, w, h = box
rect = patches.Rectangle((x, y), w - x, h - y, linewidth=2, edgecolor='green', facecolor='none')
ax.add_patch(rect)
plt.savefig('/content/output_image.png')
plt.show()

# Count the number of detected objects
num_objects_detected = len(selected_boxes)

# Calculate Count Accuracy
count_accuracy = num_objects_detected / ground_truth_count * 100

# Calculate Mean Absolute Percentage Error (MAPE)
mape = abs(num_objects_detected - ground_truth_count) / ground_truth_count * 100
print(f'Number of Fruits Detected: {num_objects_detected}')
print(f'Ground Truth Count: {ground_truth_count}')
print(f'Count Accuracy: {count_accuracy:.2f}%')
print(f'MAPE: {mape:.2f}%')
return num_objects_detected, count_accuracy, mape

## YOLOv5
! pip install ultralytics

## YOLOv5
import torch
from PIL import Image
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import yaml  # Make sure to import the yaml module
from IPython.display import display

# Load YOLOv5 model
model = torch.hub.load('ultralytics/yolov5:v6.0', 'yolov5s')

# Load image
image_path = '/content/apple test.jpg'  # Replace with the path to your image
image = Image.open(image_path)

# Perform inference
results = model(image)

# Extract class names from the model
class_names = model.names

# Display results with bounding boxes and predicted classes
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
ax.imshow(image)

# Check if objects are detected before drawing bounding boxes
if len(results.xyxy[0]) > 0:
    max_confidence_box = max(results.xyxy[0].cpu(), key=lambda box: box[4])
    x, y, w, h, max_conf, cls = max_confidence_box

    # Convert the class number to the class name
    class_name = class_names[int(cls)]

    # Draw the rectangle
    rect = Rectangle((x, y), w - x, h - y, fill=False, color='green', linewidth=2)
    ax.add_patch(rect)

    # Add text label
    plt.text(x, y, f'Class {class_name}', color='red', fontsize=10, bbox=dict(facecolor='white', alpha=0.8))

    # Save the figure to a file
    plt.savefig('/content/output_image.png')

    # Display the image
    display(fig)

    # Count the number of detected objects
    num_objects_detected = len(results.xyxy[0])

    # Provide the ground truth count (replace with your actual ground truth count)
    ground_truth_count = 58 # Replace with the actual ground truth count

    # Calculate Count Accuracy
    count_accuracy = num_objects_detected / ground_truth_count * 100

    # Calculate Mean Absolute Percentage Error (MAPE)
    mape = abs(num_objects_detected - ground_truth_count) / ground_truth_count * 100

    print(f'Number of Fruits Detected: {num_objects_detected}')
    print(f'Ground Truth Count: {ground_truth_count}')
    print(f'Count Accuracy: {count_accuracy:.2f}%')
    print(f'MAPE: {mape:.2f}%')

else:
    print("No objects detected.")

# Commented out IPython magic to ensure Python compatibility.
## PERFORMANCE GRAPH
import torch
from PIL import Image
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from torchvision.models.detection import fasterrcnn_resnet50_fpn
import torchvision.transforms as T
from IPython.display import display
import yaml
from pathlib import Path

# Function to count fruits using Faster R-CNN
def count_fruits_rcnn(image_path, ground_truth_count=8, confidence_threshold=0.5):
    # Load the image
    image = Image.open(image_path).convert("RGB")

    # Define the transformation
    transform = T.Compose([T.ToTensor()])

    # Preprocess the image
    input_image = transform(image)
    input_image = input_image.unsqueeze(0)  # Add a batch dimension

    # Load the Faster R-CNN model
    model = fasterrcnn_resnet50_fpn(pretrained=True)
    model.eval()

    # Perform inference
    with torch.no_grad():
        predictions = model(input_image)

    # Filter predictions based on confidence threshold
    boxes = predictions[0]['boxes']
    scores = predictions[0]['scores']
    selected_boxes = boxes[scores > confidence_threshold]

    # Count the number of detected objects
    num_objects_detected = len(selected_boxes)

    return num_objects_detected


# Function to count fruits using YOLOv5
def count_fruits_yolov5(image_path, ground_truth_count=8):
    # Load YOLOv5 model
    model = torch.hub.load('ultralytics/yolov5:v6.0', 'yolov5s', pretrained=True)

    # Load image
    image = Image.open(image_path)

    # Perform inference
    results = model(image)

    # Count the number of detected objects
    num_objects_detected = len(results.xyxy[0])

    return num_objects_detected


# Compare the accuracy of Faster R-CNN and YOLOv5
def compare_accuracy(image_path, ground_truth_count=8, confidence_threshold=0.5):
    num_objects_rcnn = count_fruits_rcnn(image_path, ground_truth_count, confidence_threshold)
    num_objects_yolov5 = count_fruits_yolov5(image_path, ground_truth_count)

    # Calculate Count Accuracy for each model
    count_accuracy_rcnn = num_objects_rcnn / ground_truth_count * 100
    count_accuracy_yolov5 = num_objects_yolov5 / ground_truth_count * 100

    return count_accuracy_rcnn, count_accuracy_yolov5


# Example usage
image_path = '/content/apple test.jpg'  # Replace with the path to your image
ground_truth_count = 58  # Replace with the actual ground truth count

# Compare accuracy
confidence_threshold = 0.5  # Set the confidence threshold for Faster R-CNN
count_accuracy_rcnn, count_accuracy_yolov5 = compare_accuracy(image_path, ground_truth_count, confidence_threshold)

# ... (previous code)

# Plotting
models = ['Faster R-CNN', 'YOLOv5']
accuracies = [float(count_accuracy_rcnn), float(count_accuracy_yolov5)]

plt.figure(figsize=(8, 6))
plt.bar(models, accuracies, color=['blue', 'orange'])
plt.ylim(0, 100)
plt.ylabel('Count Accuracy (%)')
plt.title('Object Detection Model Comparison')

# Convert tensor values to Python floats when plotting text annotations
for i, accuracy in enumerate(accuracies):
    plt.text(i, float(accuracy) + 1, f'{float(accuracy):.2f}%', ha='center', va='bottom', fontsize=10)

# Use the magic command to display the plot in Colab
# %matplotlib inline
plt.show()

import torchvision.models.detection as detection_models
import torch

def get_model_info(model):
    # Get the model architecture name
    model_name = model.__class__.__name__

    # Check if the model has a backbone
    if hasattr(model, 'backbone'):
        backbone_name = model.backbone.body.__class__.__name__
    else:
        backbone_name = "N/A"

    # Check if the model has ROI Heads
    has_roi_heads = hasattr(model, 'roi_heads')

    return {
        'Model Name': model_name,
        'Backbone': backbone_name,
        'Has ROI Heads': has_roi_heads,
    }

# Load Faster R-CNN model
faster_rcnn_model = detection_models.fasterrcnn_resnet50_fpn(pretrained=True)
faster_rcnn_info = get_model_info(faster_rcnn_model)

# Load YOLOv5 model
yolov5_model = torch.hub.load('ultralytics/yolov5:v6.0', 'yolov5s', pretrained=True).model
yolov5_info = get_model_info(yolov5_model)

# Print information for comparison
print("\n--- Faster R-CNN Model Information ---")
for key, value in faster_rcnn_info.items():
    print(f"{key}: {value}")

print("\n--- YOLOv5 Model Information ---")
for key, value in yolov5_info.items():
    print(f"{key}: {value}")